---
title: "Classifiers"
output: html_notebook
---

```{r include = FALSE}
library(mosaic)
```

## Classification overview

* Response variable: categorical.  Typically just a few levels: 2 or 3.
* Two types of outputs from classification models:     
    1. The predicted category given the inputs
    2. Probability of each category given the inputs
        - Type (2) can be fitted with maximum likelihood.
* Trade-offs:     
    * Flexibility vs interpretability
    * Accuracy vs bias
* Four model architectures
    1. Logistic regression.  Especially important for interpretability.
    2. Linear discriminant analysis
    3. Quadratic discriminant analysis
    4. K nearest neighbors

## Day 10 preview

1. Probability and odds    
    * [Theme Song](https://www.youtube.com/watch?v=RthEYvh6aMM)
    * [Making book](http://en.wikipedia.org/wiki/Dutch_book)
2. Multivariate gaussians (Maybe)
3. Programming activity: Poker hands

## Probability and odds

Probability $p(event)$ is a number between zero and one.

Simple way to make a probability model for yes/no variable: encode outcome as zero and one, use regression.
```{r}
Whickham$alive <- as.numeric(with(Whickham, outcome == "Alive"))
```

Model of mortality in Whickham
```{r}
res <- mean( alive ~ smoker, data=Whickham)
res
res / (1-res)
```

```{r}
mod2 <- glm(alive ~ age, data=Whickham, family = "binomial")
f <- makeFun(mod2)
plotFun(f(age) ~ age, age.lim = c(20,100))
plotPoints(jitter(alive) ~ age, data=Whickham, add=TRUE,
           pch=20, alpha=.3)
```

If we're going to use likelihood to fit, the estimated probability can't be $\leq 0$.

## Log Odds

Gerolamo Cardano (1501-1576) defined *odds* as the ratio of favorable to unfavorable outcomes.

For an event whose *probability* is $p$, it's *odds* are $w = \frac{p}{1-p}$.

A probability is a number between 0 and one.

An odds is a ratio of two positive numbers. 5:9, 9:5, etc.

"Odds are against it," could be taken to mean that the odds is less than 1.  More unfavorable outcomes than favorable ones.

Given odds $w$, the probability is $p = \frac{w}{1+w}$.  There's a one-to-one correspondence between probability and odds.

The log odds is a number between $-\infty$ and $\infty$.  

## Why use odds?

**Making Book**

Several horses in a race.  People bet on each one amounts $H_i$.

What should be the winnings when horse $j$ wins? Payoff means you get your original stake back plus your winnings.

If it's arranged to pay winnings of     
$\sum{i \neq j} \frac{H_i}{H_j}$ + the amount $H_j$   
the net income will be zero for the bookie.

*Shaving the odds* means to pay less than the zero-net-income winnings.  

**Link function**

You can build a linear regression to predict the log odds, $\ln w$.  The output of the linear regression is free to range from $-\infty$ to $\infty$.  Then, to measure likelihood, unlog to get odds $w$, then $p = \frac{w}{1+w}$.

## Use of glm() 

Response should be 0 or 1.  We don't take the log odds of the response.  Instead, the likelihood is    
- $p$ if the outcome is 1
- $1-p$ if the outcome is 0

Multiply these together of all the cases to get the total likelihood.

## Interpretation of coefficients

Each adds to the log odds in the normal, linear regression way.  Negative means less likely; positive more likely.  

# Multivariate Gaussian

## Joint probabilities and classification

\newcommand{\defeq}{\mathrel{\mathop:}=} 

Suppose we have $K$ classes, $A_1, A_2, \ldots, A_K$.  We also have a set of inputs $x_1, x_2, \ldots, x_p \defeq {\mathbf x}$. 

We observe ${\mathbf x}$ and we want to know $p(A_j | {\mathbf x})$.

To set things up so that we can find $p(A_j | {\mathbf x})$, we collect a lot of objects of class $A_j$ and measure ${\mathbf x}$ from each of them.  We use this to create a model probability:

$$p({\mathbf x} | A_j)$$

## Independent variables $x_i$

* Describing dependence

```{r}
x1 = runif(1000)
x2 = rnorm(1000, mean=3*x1+2, sd=x1)
plot(x1, x2)
```

* Linear correlations and the Gaussian

Remember the univariate Gaussian with parameters $\mu$ and $\sigma^2$:    
$$\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)$$

Situation: Build a classifier.  We measure some features and want to say which group a case refers to.

Specific example: Based on the `ISLR::Default` data, find the probability of a person defaulting on a loan given their `income` and `balance`. 

```{r fig.width=8, fig.height=8, out.width = "6in"}
names(Default)
ggplot(Default, 
       aes(x = income, y = balance, alpha = default, color = default)) + 
  geom_point()
```

We were looking at the likelihood: prob(observation | class)

Note: Likelihood itself won't do a very good job, since defaults are relatively uncommon. That is, p(default) $\ll$ p(not).

A standard (but not necessarily good) model of a distribution is a multivariate Gaussian.

## Univariate Gaussian

$$p(x) = \underbrace{\frac{1}{\sqrt{2 \pi \sigma^2}}}_{Normalization} \underbrace{\exp(- \frac{(x-m)^2}{2 \sigma^2})}_{Shape}$$

Imagine that we have another variable $z = x/3$. Geometrically, $z$ is a stretched out version of $x$, stretched by a factor of 3.  The distribution is

$$p(z) = \underbrace{\frac{1}{\sqrt{2 \pi (3\sigma)^2}}}_{Normalization} \underbrace{\exp(- \frac{(x-m)^2}{2 (3\sigma)^2})}_{Shape}$$

Note how the normalization changes.  $p(z)$ is broader than $p(x)$, so it must also be shorter.

## Uncorrelated bivariate gaussian

For independent RVs x and y, p(xy) = p(x)p(y).  Show that the normalization is
$\frac{1}{2 \pi \sigma_x \sigma_y}$.

The sigmas multiply in the normalization, like the area of something being stretched out in two orthogonal directions.

## Bivariate normal distribution with correlations


$$f(x,y) =
      \frac{1}{2 \pi  \sigma_X \sigma_Y \sqrt{1-\rho^2}}
      \exp\left(
        -\frac{1}{2(1-\rho^2)}\left[
          \frac{(x-\mu_X)^2}{\sigma_X^2} +
          \frac{(y-\mu_Y)^2}{\sigma_Y^2} -
          \frac{2\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X \sigma_Y}
        \right]
      \right)$$

If $\rho > 0$ and $x$ and $y$ are both above their respective means, the correlation term makes the result *less* surprising: a larger probability.

Or, for multiple dimensions


$$(2\pi)^{-\frac{k}{2}}|\boldsymbol\Sigma|^{-\frac{1}{2}}\, \exp\left( -\frac{1}{2}(\mathbf{x}-\boldsymbol\mu)'\boldsymbol\Sigma^{-1}(\mathbf{x}-\boldsymbol\mu) \right)$$

As an amplitude plot
![amplitude plot](https://upload.wikimedia.org/wikipedia/commons/5/57/Multivariate_Gaussian.png)

Showing marginals and 3-$\sigma$ contour 
![marginals](https://upload.wikimedia.org/wikipedia/commons/8/8e/MultivariateNormal.png)


## Determinant gives area of a parallelogram

Look at the stretching that goes on

![Proof without words](http://i.stack.imgur.com/gCaz3.png)
