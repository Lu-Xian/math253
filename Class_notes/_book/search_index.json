[
["preface.html", "Notes for Math 253: Statistical Computing and Machine Learning Preface Math 253 and the Macalester statistics curriculum These notes are written in Bookdown", " Notes for Math 253: Statistical Computing and Machine Learning Daniel Kaplan Fall 2016, Macalester College Preface Math 253 and the Macalester statistics curriculum Math 253, Statistical Computing and Machine Learning, is a course at Macalester College. The course comes early in our curriculum for the statistics major. Currently, that curriculum looks like: Comp 110, Data Computing, a no-prerequisite course in data wrangling and visualization. The main tools in the course are R, dplyr, and ggplot. Math 125, Epidemiology. This course also has no pre-requisites. It’s designed to teach quantitative and statistical literacy in the context of public health and decision making. Unlike the other courses in our curriculum, Math 125 is not computer intensive. This is an elective for the statistics major. Math 155, Introduction to Statistical Modeling. This is our main entry-level statistics course. It also has no pre-requisites, but we encourage students to take a course in our calculus sequence: Applied Multivariate Calculus I, II, and III. The course covers important concepts in modeling: model architectures and fitting using mainly linear and logistic regression, covariation and adjustment, interpretation of model coefficients, inference techniques such as analysis of variance and co-variance and the ways these techniques can inform the decisions needed to build useful models, causality including techniques for making reasonable conclusions about causation from observational data. The course makes very extensive and intensive use of R and the mosaic package for R. This course, Math 253, Statistical Computing and Machine Learning. Math 253 introduces a broader set of model architectures (e.g. those associated with “machine learning” such as support vector machines) and the trade-offs that make machine learning a human skill rather than a push button mechanism. The main text is Introduction to Statistical Learning in R. Computing in R is used intensively in the course. Math 155 is a pre-requisite. The computing used in that course is the only pre-requisite. A small part of Math 253 is given to basic programming in R, but the large majority is about the mathematical and statistical concepts involved in learning from data and the exercise of a variety of architectures for classification and regression models. Math 253 is placed intentionally in the middle of our statistics curriculum to make it accessible to many students who are interested and may have use for the techniques, but whom are not primarily interested in statistics per se. Upper level courses including: Math 454: Bayesian statistics Math 453: Biostatistics Math 454: Mathematical statistics Supporting courses including: Math 354: Probability Math 135: Applied Multivariate Calculus I Math 236: Linear algebra Comp 123: Core conceptsin computer science Electives such as Math 432: Mathematical modeling Math/Comp 365: Numerical linear algebra Comp 302: Introduction to database management systems These notes are written in Bookdown The document uses an elaboration on R/Markdown, called “Bookdown.” I don’t yet know if this will be a good way to maintain and distribute class notes. Some advantages: All the notes are in one place. Students and other instructors can clone the notes for their own uses. People spotting mistakes can contribute corrections via the “pull request” mechanism supported by Bookdown working with GitHub. Multiple instructors can contribute to the notes via GitHub. This is the stated purpose behind Bookdown — allowing book-length publications to be authored by many authors. Disadvantages: The notes are always a work in progress. Don’t be mislead by the polish that RMarkdown and Bookdown give to the notes. "],
["introduction.html", "Topic 1 Introduction 1.1 Statistical and Machine Learning 1.2 Theoretical concepts ISL §2.1 1.3 In-class programming activity 1.4 Review from Day 1 1.5 Day 2 theory: accuracy, precision, and bias 1.6 Programming Basics I: Names, classes, and objects 1.7 Day 2 in-class programming activity 1.8 Review of Day 2 1.9 A Classifier example", " Topic 1 Introduction Subjects Overview of statistical learning. Getting started with R, RStudio, and RMarkdown Reading: Chapter 2 of ISL Programming basics 1: Names, classes, and objects 1.1 Statistical and Machine Learning The two terms, “statistical learning” and “machine learning,” reflect mainly the artificialities of academic disciplines. Statisticians focus on the statistical aspects of the problems Computer scientists are interested in “machines”, including hardware and software. “Data Science” is a new term that reflects the reality that both statistical and machine learning are about data. Techniques and concepts from both statistics and computer science are essential. 1.1.1 Statistics concepts Sampling variability Bias and variance Characterization of precision Function estimation frameworks, e.g. generalized linear models Assumed probability models Prior and posterior probabilities (Bayesian framework) 1.1.2 Computing concepts Algorithms Iteration Simulation Function estimation frameworks, e.g. classification trees, support vector machines, artificial intelligence techniques Kalman filters 1.1.3 Cross fertilization Assumed probability models supplanted by simulation Randomization and iteration Cross validation Bootstrapping Model interpretibility Rather than an emphasis on the output of a function, interest in what the function has to say about how the world works. 1.1.4 Example 1: Machine translation of natural languages Computer scientists took this on. Identification of grammatical structures and tagging text. Dictionaries of single-word equivalents, common phrases. Story from early days of machine translation: Start with English: “The spirit is willing, but the flesh is weak.” Translate into Russian Translate back into English. Result: “The vodka is good, but the meat is rotten.” Statistical approach: Take a large sample of short phrases in language A and their human translation into language B: the dictionary Find simple measures of similarity between phrases in language A (e.g. de-stemmed words in common) Take new phrase in language A, look up it’s closest match in the dictionary phrases in language A. Translation is the corresponding dictionary entry in language B Where did the sample of phrases come from? European Union documents routinely translated into all the member languages. Humans mark correspondence. “Mechanical Turk” dispersal of small work tasks. Result: Google translate. 1.1.5 Example 2: From library catalogs to latent semantic indexing Early days: computer systems with key words and search systems (as in library catalogs) Now: dimension reduction (e.g. singular value decomposition), angle between specific documents and what might be called “eigen-documents” Result: Google search 1.2 Theoretical concepts ISL §2.1 1.2.1 Prediction versus inference Black-box predictions. If the box works, why worry about what’s inside? Example: Malignancy of cancer from appearance of cells. Works for guiding treatment. Does it matter why malignant cells have the appearance they do? Story: Mid-1980s. Heart rate variability spectral analysis and holter monitors. (Holters were cassette tape recorders set to record ECG very, very slowly. Spectral analysis breaks down the overal signal into periodic components.) Very large spike at 0.03 Hz seen in people who will soon die. Causal influences. We want to use observations to inform our understanding of what influences what. Story continued: The very large spike was the “wow and flutter” in the cassette tape mechanism. This had an exact periodicity: a spike in the spectrum. If the person was sick, their heart rate was steady: they had no capacity to vary it as other conditions in the body (arterial compliance, venus tone) called for. Understanding what happens in cardiac illness is, in part, about understanding how the various control systems interact. 1.2.2 Accuracy versus interpretability Sometimes “flexibility” is used instead of “accuracy”. Not flexible: Figure 1.1: Individual fits miss how the explanatory variables interact. ISL Figure 2.1 Flexible: Figure 1.2: Such detailed patterns are more closely associated with physical science data than with social/economic data. ISL Figure 2.2 And in multiple variables: Not flexible: Figure 1.3: ISL Figure 2.4 Flexible: Figure 1.4: ISL Figure 2.6 A quick characterization of several model architectures (which they call “statistical learning methods”) Figure 1.5: ISL Figure 2.7 1.2.3 Reducible versus irreducible error What does this mean? (from p. 19) \\[\\begin{array}{rcl} E(Y - \\hat{Y})^2 &amp; = &amp; E[f(X) + \\epsilon - \\hat{f}(X)]^2\\\\ &amp; = &amp; \\underbrace{[f(X) - \\hat{f}(X)]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible}\\\\ \\end{array}\\] Notation: \\(X\\) — the inputs that determine the output \\(Y\\). \\(Y\\) — the output, that is, the quantity we want to predict \\(\\hat{Y}\\) — our prediction hat means estimated, no hat means “real” (whatever that might mean) \\(E(Y - \\hat{Y})^2\\) — the mean of the square difference between our prediction and the “real” value. \\(E\\) means “expectation value.” \\(f(X)\\) — what \\(Y\\) would be, ideally, for a given \\(X\\) \\(\\hat{f}(X)\\) — our estimate of \\(f(X)\\) \\(\\epsilon\\) — but \\(Y\\) is subject to other, “random” influences. \\(\\epsilon\\) represents these. \\(\\epsilon\\) is a misleading notation because it may not be at all small in practice. But \\(\\epsilon\\) is alway centered on zero (by definition). \\(|f(X) - \\hat{f}(X)|\\) — the magnitude of the difference between the “real” \\(f()\\) and our estimate. This can be made small by collecting more data using a more flexible model expanding the set of inputs considered \\(Var(\\epsilon)\\) — the “variance” of \\(\\epsilon\\). This is the mean square of \\(\\epsilon\\), that is, \\(E(\\epsilon^2)\\). 1.2.4 Regression versus classification Regression: quantitative response (value, probability, count, …) Classification: categorical response with more than two categories. (When there are just two categories, regression (e.g. logistic regression) does the job.) 1.2.5 Supervised versus unsupervised Demographics groups in marketing. Poverty vs middle-class Political beliefs … left vs right? Figure 1.6: ISL Figure 2.8 1.3 In-class programming activity Day 1 activity 1.4 Review from Day 1 Supervised vs unsupervised We will be doing only supervised learning until late in the course Supervised learning: We have a set of cases, \\(i = 1, 2, 3, \\ldots, n\\), called the training data. For each case, we have an input \\({\\mathbf X_i}\\) consisting potentially of several variables measured on that case. The subscript \\(i\\) in \\({\\mathbf X_i}\\) means that we have one \\({\\mathbf X}\\) for each case. The boldface \\({\\bf X}\\) means that the input can be multi-dimensional, that is, consisting of multiple variables. For each case, we have an output \\(Y_i\\). We want to learn the overall pattern of the relationship between the inputs \\({\\mathbf X}\\) and the outputs \\(Y\\), not just for our \\(n\\) training cases, but for potential cases that we have not encountered yet. These as yet unencountered cases are thought of as the testing data. We are going to represent the pattern with a function \\(\\hat{f} : {\\bf X} \\rightarrow Y\\). Sometimes I’ll use the word model instead of function. A model is a representation for a purpose. A function is a kind of representation. So some models involve functions. That’s the kind we’ll focus on in this course. I say “model” out of habit, but it’s a good habit that reminds us that the purpose of the function is important and we should know what that purpose is when we undertake learning. Regression vs classification Different kinds of functions. A classifier has output as a categorical level. A regression has output as a number. Prediction vs inference Two different kinds of purpose. There may well be different kinds of functions best suited to each purpose. Accuracy vs interpretability We always want models to be accurate. Whether we need to be able to interpret the model depends on our overall purpose. Reducible error vs irreducible error It’s good to know how accurate our models can get. That gives a goal for trying out different types of models to know when we don’t need to keep searching. 1.5 Day 2 theory: accuracy, precision, and bias 1.5.1 Figure 2.10 In constructing a theory, it’s good to have a system you can play with where you know exactly what is going on: e.g. a simulation. The dark blue line in the left panel is a function the authors created for use in a simulation: Figure 1.7: ISL Figure 2.9 The dots are data they generated from evaluating the function at a few dozen values of \\(x\\) and adding noise to each result. The difference between the dots’ vertical position and the function value is the residual, which they are calling the error. The mean square error MSE is \\[\\mbox{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - f(x_i))^2\\] Take this notation apart. What’s \\(n\\)? What’s \\(i\\)? Suppose that \\(f(x)\\) were constant. In that situation, what kind of statistical quantity does this resemble? In actual practice, we don’t know \\(f(x)\\). (Indeed, it’s a matter of philosophy whether this is an \\(f(x)\\) — it’s a kind of Platonic form.) Here we know \\(f(x)\\) because we are playing a game: running a simulation. Looking again at the left panel in Figure 2.9, you can see three different functions that they have fitted to the data. It’s not important right now, but you might as well know what these model architectures are: Linear regression line (orange) Smoothing splines (green and light blue). A smoothing spline is a functional form with a parameter: the smoothness. The green function is less smooth than the light blue function. That smoothness measure can also be applied to the linear regression form Each of these three functions were fitted to the data. Another word for fitted is trained. As such, we use the term training error for the difference between the data points and the fitted functions. Also, because the functions are not the Platonic \\(f(x)\\), they are written \\(\\hat{f}(x)\\). For each of the functions, the training MSE is \\[\\mbox{Training MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\] Right panel of the graph is something completely different: both the axes are different than in the left panel. x-axis: the smoothness of the functions. This is labelled flexibility. The three x positions correspond to the smoothness of the three models. This is measured as the effective number of parameters of the function. Why does the straight-line function have a smoothness of 2? y-axis: the MSE The dots connected by the gray curve show the training MSE of the three models. The dots connected by the orange curve show the testing MSE of the three models. The continuous curves were constructed by calculating the MSE for many more values of smoothness than shown in the left panel. How did they measure the training MSE? 1.5.2 Another example: A smoother simulated \\(f(x)\\). Figure 1.8: ISL Figure 2.10 What’s different between the right panel of 2.9 and that of 2.10? 1.5.3 What’s the “best” of these models? When examining training MSE, the more flexible model has the smaller MSE. This answer is pre-ordained, regardless of the actual shape of the Platonic \\(f(x)\\). In traditional regression, we use ANOVA or *adjusted$ \\(R^2\\) to help avoid this inevitability that more complicated models will be closer to the training data. Another approach to this is to use testing MSE rather than training MSE. So pick the model with flexibility at the bottom of the U-shaped testing MSE curve. 1.5.4 Why is testing MSE U-shaped? Bias: how far \\(\\hat{f}(x)\\) is from \\(f(x)\\) Variance: how much \\(\\hat{f}\\) would vary among different randomly selected possible training samples. In traditional regression, we get at the variance by using confidence intervals on parameters. The broader the confidence interval, the higher the variation from random sample to random sample. These confidence intervals come from normal theory or from bootstrapping. Bootstrapping is a simulation of the variation in model fit due to training data. Bias decreases with higher flexibility. Variance tends to increase with higher flexibility. Irreducible error is constant. Figure 1.9: ISL Figure 2.12 1.5.5 Measuring the variance of independent sources of variation Simulation: Add three different sources of variation. The width of the individual sources is measured by the standard deviation sd=. require(mosaic) n &lt;- 1000 sd( rnorm(1000, sd=3) + rnorm(1000, sd=1) + rnorm(1000, sd=2) ) ## [1] 3.666602 Divide into small groups and construct a theory about how the variation in the individual components relates to the variation in the whole. test whether your theory works for other random distributions, e.g. rexp() 1.5.6 Equation 2.7 \\[E( y - \\hat{f}(x) )^2 = \\mbox{Var}(\\hat{f}(x)) + [\\mbox{Bias}(\\hat{f}(x))]^2 + \\mbox{Var}(\\epsilon)\\] Breaks down the total “error” into three independent sources of variation: How \\(y_i\\) differs from \\(f(x_i)\\). This is the irreducible noise: \\(\\epsilon\\) How \\(\\hat{f}(x_i)\\) (if fitted to the testing data) differs from \\(f(x_i)\\). This is the bias. How the particular \\(\\hat{f}(x_i)\\) fitted to the training data differs from the \\(\\hat{f}(x_i)\\) that would be the best fit to the testing data. \\[\\underbrace{E( y - \\hat{f}(x) )^2}_{\\mbox{Total error}} = \\underbrace{\\mbox{Var}(\\hat{f}(x))}_{\\mbox{source 3.}} + \\underbrace{[\\mbox{Bias}(\\hat{f}(x))]^2}_{\\mbox{source 2.}} + \\underbrace{\\mbox{Var}(\\epsilon)}_{\\mbox{source 1.}}\\] 1.6 Programming Basics I: Names, classes, and objects 1.6.1 Names Composed of letters, numbers, _ and .. - Don’t use . — it’s a bad habit. But plenty of people do. - Can’t lead with a number. - Capitalization counts. - Unquoted (… almost always) 1.6.2 Objects Information (bits) in a particular format. - Different formats for different purposes. - The format is the class() or mode(). mode is more basic than class. Assignment: Give a name to an object 1.6.3 Classes Different classes represent different kinds of things. They typically have different operations that are relevant. 1-dimensional homogeneous collections of numbers or of character strings. These collections are called vectors 1-dimensional means you need only one index to refer to a specific element length(), which can be zero. Operations, e.g. sum(), mean(), max() … 2-dimensional homogeneous collections of numbers or of character strings. These collections are called matrices 2-dimensional means you need two indices to refer to a specific element. dim() Operations, e.g., t(), colSums(), rowSums(), %*%, … 1-dimensional heterogeneous collections: lists Data frames Functions 1.7 Day 2 in-class programming activity Day 2 activity 1.8 Review of Day 2 \\(f({\\mathbf X})\\) versus \\(\\hat{f}({\\mbox{X}})\\): Platonic idea versus what we get out of the training data. Quip: “The hat means there’s a person underneath the model.” Mean Square Error — like the standard deviation of residuals Training vs testing data Smoothness, a.k.a. flexibility, model degrees of freedom More flexibility \\(\\rightarrow\\) better training MSE Components of MSE Irreducible random noise: \\(\\epsilon\\) Bias: \\(f({\\mathbf X}) - \\hat{f}({\\mathbf X})\\) Caused by too much smoothness Caused by omitting a relevant variable Caused by including an irrelevant variable \\(Var(\\hat{f}({\\mathbf X}))\\) — how much \\(\\hat{f}\\) varies from one possible training set to another. Increased by too many degrees of freedom: overfitting Increased by collinearity and multi-collinearity. Increased by large \\(\\epsilon\\) Decreased by large \\(n\\) 1.9 A Classifier example A classification setting: Blood cell counts. Build a machine which takes a small blood sample and examines and classifies individual white blood cells. Figure 1.10: Blood cell classification The classification is to be based on two measured inputs, shown on the x- and y-axes. Training data has been developed where the cell was classified “by hand.” In medicine, this is sometimes called the gold standard. The gold standard is sometimes not very accurate. Here, each cell is one dot. The color is the type of the cell: granulocytes, lymphocytes, monocytes, … "],
["linear-regression.html", "Topic 2 Linear Regression 2.1 Day 4 Overview 2.2 Small data 2.3 Selecting model terms 2.4 Programming basics: Graphics 2.5 In-class programming activity 2.6 Review of Day 4 2.7 Regression and Interpretability 2.8 Measuring Accuracy of the Model 2.9 Bias of the model 2.10 Forward, backward and mixed selection 2.11 In-class programming activity", " Topic 2 Linear Regression 2.1 Day 4 Overview The linear model (e.g. what lm() does) A variety of questions relevant to different purposes, e.g. how good will a prediction be? what’s the strength of an effect? is there synergy between different factors? 2.1.1 ISL book’s statement on why to study linear regression “Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described … later …, linear regression is still a useful and widely used statistical learning method. Moreover, it serves as a good jumping-off point for newer approaches…. Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated.” Concepts from linear regression: Choice of explanatory variables and model term (such as interaction). “Degrees of freedom” Ease of interpretability of coefficients and their standard errors. 2.2 Small data The regression techniques were developed in an era of small data, such as that that might be written in a lab notebook or field journal. As a result: Emphasis on very simple descriptions, such as means, differences between means, simple regression. Theoretical concern with details of distributions, such as the t-distribution. No division into training and testing data. Data are too valuable to test! (Ironic, given the importance of replicability in the theory of the scientific method.) As a consequence of (3), there’s a great deal of concern about assumptions, e.g. linearity of \\(f({\\mathbf X})\\) structure of \\(\\epsilon\\): IID — Independent and Identically Distributed uncorrelated between cases each is a draw from the same distribution. 2.3 Selecting model terms The regression techniques Heirarchical principal Increase in \\(R^2\\) 2.3.1 Theory of whole-model ANOVA. Standard measure: \\(\\frac{\\mbox{Explained amount}}{\\mbox{Unexplained amount}}\\) Examples: Standard error of mean: \\(\\frac{\\hat{\\mu}}{\\sigma / n}\\) – note the \\(n\\). t statistic on difference between two means: \\(\\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2}{\\sigma / (n-1)}\\) F statistic: \\(\\frac{SS / df1}{SSR / df2}\\) df1 is the number of degrees of freedom involved by the model or model term under consideration. df2 is \\(n - (p - 1)\\) where \\(p\\) is the total degrees of freedom in the model. (I called this \\(m\\) in the Math 155 book.) The intercept is what the \\(-1\\) is about: the intercept can never account for case-to-case variation. Trade-off between eating variance and consuming degrees of freedom. The \\(R^2\\) versus \\(p\\) picture. Adjusted \\(R^2\\) Whole model ANOVA. ANOVA on model parts 2.4 Programming basics: Graphics Basic functions: Create a frame: plot(). Blank frame: plot( , type=&quot;n&quot;) set axis limits, Dots: points(x, y), pch=20 Lines: lines(x, y) — with NA for line breaks Polygons: polygon(x, y) — like lines but connects first to last. fill Color, size, … rgb(r, g, b, alpha), “tomato” 2.5 In-class programming activity Day 4 activity Drawing a histogram. 2.6 Review of Day 4 2.6.1 Graphics basics API for graphics: plot(), points(), lines(), polygon(), text(), … Create a plotting frame: plot() Write a function that makes this more convenient to use. What features would you like. blank_frame &lt;- function(xlim, ylim) { } Write a function to draw a circle. What do you want the interface to look like? What arguments are essential? What options are nice to have? 2.7 Regression and Interpretability Regression models are generally constructed for the sake of interpretability: Global linearity Coefficients are indication of effect size. The coefficients have physical units. Term by term indication of statistical significance 2.8 Measuring Accuracy of the Model \\(R^2\\) - Var(fitted)/Var(response) Adjusted \\(R^2\\) - takes into account estimate of average increase in \\(R^2\\) per junk degree of freedom Residual Standard Error - Sqrt of Average square error per residual degree of freedom. The sqrt of the mean square for residuals in ANOVA 2.9 Bias of the model Perhaps effect of TV goes as sqrt(money) as media get saturated? Perhaps there is a synergy that wasn’t included in the model? Whole model ANOVA. ANOVA on model parts Adjusted \\(R^2\\) Run an example on College data from ISLR package data(College, package=&quot;ISLR&quot;) College$Yield &lt;- with(College, Enroll/Accept) mod1 &lt;- lm(Yield ~ Outstate + Grad.Rate + Top25perc, data=College) What variables matter? How good are the predictions? How strong are the effects? 2.10 Forward, backward and mixed selection Use the College model to demonstrate each of the approaches by hand. Start with pairs() or write an lapply() for the correlation with Yield? Create a whole bunch of model terms “main” effects “interaction” effects nonlinear transformations: powers, logs, sqrt, steps, … categorical variables Result: a set of \\(k\\) vectors that we’re interested to use in our model. Considerations: not all of the \\(k\\) vectors may pull their weight two or more vectors may overlap in how they eat up variance Algorithmic approaches: Try all combinations, pick the best one. computationally expensive/impossible \\(2^k\\) possibilities what’s the sensitivity of the process to the choice of training data? “Greedy” approaches 2.11 In-class programming activity Day 5 activity Drawing a histogram. "],
["instructions-for-the-publishing-system-bookdown.html", "Instructions for the publishing system: Bookdown", " Instructions for the publishing system: Bookdown You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter ??. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2016) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). "]
]
