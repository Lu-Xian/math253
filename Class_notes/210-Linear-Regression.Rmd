# Linear Regression {#linear-regression}


```{r include=FALSE}
require(mosaic)
```

## Day 4 Overview

* The linear model (e.g. what `lm()` does)
* A variety of questions relevant to different purposes, e.g.
    - how good will a prediction be?
    - what's the strength of an effect?
    - is there synergy between different factors?

### ISL book's statement on why to study linear regression

> "Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described ... later ..., linear regression is still a useful and widely used statistical learning method.  Moreover, it serves as a good jumping-off point for newer approaches....  Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated."

Concepts from linear regression:

* Choice of explanatory variables and model term (such as interaction).
* "Degrees of freedom"
* Ease of interpretability of coefficients and their standard errors.

## Small data

The regression techniques were developed in an era of small data, such as that that might be written in a lab notebook or field journal.  As a result:

1. Emphasis on very simple descriptions, such as means, differences between means, simple regression.
2. Theoretical concern with details of distributions, such as the t-distribution.
3. No division into training and testing data.  Data are too valuable to test! (Ironic, given the importance of replicability in the theory of the scientific method.)

As a consequence of (3), there's a great deal of concern about *assumptions*, e.g. 

* linearity of $f({\mathbf X})$
* structure of $\epsilon$: IID --- Independent and Identically Distributed
    - uncorrelated between cases
    - each is a draw from the same distribution.

## Selecting model terms

The regression techniques 

- Heirarchical principal
- Increase in $R^2$


### Theory of whole-model ANOVA.

Standard measure: $\frac{\mbox{Explained amount}}{\mbox{Unexplained amount}}$

Examples:

- Standard error of mean:  $\frac{\hat{\mu}}{\sigma / n}$ -- note the $n$.
- t statistic on difference between two means: $\frac{\hat{\mu}_1 - \hat{\mu}_2}{\sigma / (n-1)}$
- F statistic: $\frac{SS / df1}{SSR / df2}$
    - df1 is the number of degrees of freedom involved by the model or model term under consideration.
    - df2 is $n - (p - 1)$ where $p$ is the total degrees of freedom in the model.  (I called this $m$ in the Math 155 book.)  The intercept is what the $-1$ is about: the intercept *can never* account for case-to-case variation.

Trade-off between eating variance and consuming degrees of freedom.

The $R^2$ versus $p$ picture.

- Adjusted $R^2$
- Whole model ANOVA.
- ANOVA on model parts


## Programming basics: Graphics

Basic functions:

1. Create a frame: `plot()`.  Blank frame: `plot( , type="n")`
    - set axis limits, 
2. Dots: `points(x, y)`, `pch=20`
3. Lines: `lines(x, y)` --- with `NA` for line breaks
4. Polygons: `polygon(x, y)` --- like lines but connects first to last.
    - fill
5. Color, size, ... `rgb(r, g, b, alpha)`, "tomato"



## In-class programming activity 

[Day 4 activity](../Daily-Programming/Day-4-Programming-Task.pdf)

Drawing a histogram.


## Review of Day 4

### Graphics basics

1. API for graphics: `plot()`, `points()`, `lines()`, `polygon()`, `text()`, ...
2. Create a plotting frame: `plot()`
    - Write a function that makes this more convenient to use. What features would you like. 
    ```{r}
    blank_frame <- function(xlim, ylim) {
      
    }
    ```
3. Write a function to draw a circle.
    - What do you want the interface to look like? What arguments are essential? What options are nice to have?
    
## Regression and Interpretability

Regression models are generally constructed for the sake of interpretability:

* Global linearity
* Coefficients are indication of effect size. The coefficients have physical units.
* Term by term indication of statistical significance

## Measuring Accuracy of the Model

* $R^2$ - Var(fitted)/Var(response)
* Adjusted $R^2$ - takes into account estimate of average increase in $R^2$ per junk degree of freedom
* Residual Standard Error - Sqrt of Average square error per residual degree of freedom. The sqrt of the mean square for residuals in ANOVA

## Bias of the model

* Perhaps effect of TV goes as sqrt(money) as media get saturated?
* Perhaps there is a synergy that wasn't included in the model?


- Whole model ANOVA.
- ANOVA on model parts
- Adjusted $R^2$

Run an example on `College` data from `ISLR` package 

```{r}
data(College, package="ISLR")
College$Yield <- with(College, Enroll/Accept)
mod1 <- lm(Yield ~ Outstate + Grad.Rate + Top25perc, data=College)
```
* What variables matter?
* How good are the predictions?
* How strong are the effects?



## Forward, backward and mixed selection

Use the `College` model to demonstrate each of the approaches by hand.  Start with `pairs()` or write an `lapply()` for the correlation with `Yield`?

Create a whole bunch of model terms

- "main" effects
- "interaction" effects
- nonlinear transformations: powers, logs, sqrt, steps, ...
- categorical variables

Result: a set of $k$ vectors that we're interested to use in our model.

Considerations:

- not all of the $k$ vectors may pull their weight
- two or more vectors may overlap in how they eat up variance

Algorithmic approaches:

- Try all combinations, pick the best one.
    - computationally expensive/impossible $2^k$ possibilities
    - what's the sensitivity of the process to the choice of training data?
- "Greedy" approaches


## In-class programming activity 

[Day 5 activity](../Daily-Programming/Day-05-Programming-Task.pdf)

Drawing a histogram.


